{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPf82LQpJIc00JZLeGEsYYu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seawavve/PeekABook/blob/main/Spark_NER_trimmed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae7PAh_Z0XZA",
        "outputId": "4f54818b-ad25-480c-ae8d-34cea91cbc54"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-08 06:32:48--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-07-08 06:32:48--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608 (1.6K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-08 06:32:49 (28.4 MB/s) - written to stdout [1608/1608]\n",
            "\n",
            "setup Colab for PySpark 3.0.3 and Spark NLP 3.1.2\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [637 kB]\n",
            "Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,221 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,658 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,780 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,418 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,188 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [910 kB]\n",
            "Get:23 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.8 kB]\n",
            "Fetched 12.1 MB in 7s (1,709 kB/s)\n",
            "Reading package lists... Done\n",
            "\u001b[K     |████████████████████████████████| 209.1MB 65kB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 30.2MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tINf3SZP0Nzy"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "def get_ann_pipeline ():\n",
        "    \n",
        "    document_assembler = DocumentAssembler() \\\n",
        "        .setInputCol(\"text\")\\\n",
        "        .setOutputCol('document')\n",
        "\n",
        "    sentence = SentenceDetector()\\\n",
        "        .setInputCols(['document'])\\\n",
        "        .setOutputCol('sentence')\\\n",
        "        .setCustomBounds(['\\n'])\n",
        "\n",
        "    tokenizer = Tokenizer() \\\n",
        "        .setInputCols([\"sentence\"]) \\\n",
        "        .setOutputCol(\"token\")\n",
        "\n",
        "    pos = PerceptronModel.pretrained() \\\n",
        "              .setInputCols([\"sentence\", \"token\"]) \\\n",
        "              .setOutputCol(\"pos\")\n",
        "    \n",
        "    embeddings = WordEmbeddingsModel.pretrained()\\\n",
        "          .setInputCols([\"sentence\", \"token\"])\\\n",
        "          .setOutputCol(\"embeddings\")\n",
        "\n",
        "    ner_model = NerDLModel.pretrained() \\\n",
        "          .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "          .setOutputCol(\"ner\")\n",
        "\n",
        "    ner_converter = NerConverter()\\\n",
        "      .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
        "      .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "    ner_pipeline = Pipeline(\n",
        "        stages = [\n",
        "            document_assembler,\n",
        "            sentence,\n",
        "            tokenizer,\n",
        "            pos,\n",
        "            embeddings,\n",
        "            ner_model,\n",
        "            ner_converter\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "    ner_pipelineFit = ner_pipeline.fit(empty_data)\n",
        "\n",
        "    ner_lp_pipeline = LightPipeline(ner_pipelineFit)\n",
        "\n",
        "    print (\"Spark NLP NER lightpipeline is created\")\n",
        "\n",
        "    return ner_lp_pipeline\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3L5-lUp0OuA",
        "outputId": "22ce669e-cd65-42fa-c217-af670b4545e6"
      },
      "source": [
        "conll_pipeline = get_ann_pipeline ()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 3.9 MB\n",
            "[OK!]\n",
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n",
            "ner_dl download started this may take some time.\n",
            "Approximate size to download 13.6 MB\n",
            "[OK!]\n",
            "Spark NLP NER lightpipeline is created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1KRI-XQ0Owv",
        "outputId": "578815ad-988c-48cb-cb10-bc03bc29f54c"
      },
      "source": [
        "parsed = conll_pipeline.annotate (\"Peter Parker Baker is in a baby blue Cadillac.\")\n",
        "\n",
        "for key in parsed.keys():\n",
        "    print(key,': ',parsed[key])\n",
        "    \n",
        "#parsed"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "document :  ['Peter Parker Baker is in a baby blue Cadillac.']\n",
            "ner_chunk :  ['Peter Parker Baker', 'Cadillac']\n",
            "pos :  ['NNP', 'NNP', 'NNP', 'VBZ', 'IN', 'DT', 'NN', 'JJ', 'NNP', '.']\n",
            "token :  ['Peter', 'Parker', 'Baker', 'is', 'in', 'a', 'baby', 'blue', 'Cadillac', '.']\n",
            "ner :  ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
            "embeddings :  ['Peter', 'Parker', 'Baker', 'is', 'in', 'a', 'baby', 'blue', 'Cadillac', '.']\n",
            "sentence :  ['Peter Parker Baker is in a baby blue Cadillac.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRU2OEM0Oz8",
        "outputId": "e05ccee0-d96b-40a2-95ba-534596f702a0"
      },
      "source": [
        "conll_lines=''\n",
        "\n",
        "for token, pos, ner in zip(parsed['token'],parsed['pos'],parsed['ner']):\n",
        "\n",
        "    conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, ner)\n",
        "\n",
        "\n",
        "print(conll_lines)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peter NNP NNP B-PER\n",
            "Parker NNP NNP I-PER\n",
            "is VBZ VBZ O\n",
            "in IN IN O\n",
            "a DT DT O\n",
            "baby NN NN O\n",
            "blue JJ JJ O\n",
            "Cadillac NNP NNP B-ORG\n",
            ". . . O\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_DB67Dy1vaG",
        "outputId": "0840d58f-6830-43bb-cacd-be9361ad5925"
      },
      "source": [
        "# csv로 자동화\n",
        "data=[]\n",
        "sentences=['Peter Parker Baker is in a baby blue Cadillac.','I love you.']\n",
        "for sentence in sentences:\n",
        "  parsed = conll_pipeline.annotate (sentence)\n",
        "  if 'B-PER' in parsed['ner']:\n",
        "    name_list=[]\n",
        "    #PER가진애들 집어넣기\n",
        "  else:\n",
        "    data.append([])\n",
        "print(data)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Peter'], ['Parker'], ['Baker'], ['Baker'], ['Baker'], ['Baker'], ['Baker'], ['Baker'], ['Baker'], ['Baker'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
